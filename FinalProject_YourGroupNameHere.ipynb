{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning-Powered PacMan: Optimizing Pellet Collection and Ghost Avoidance\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Ananya Krishnan\n",
    "- Ava Jeong\n",
    "- Charlene Hsu\n",
    "- JohnWesley Pabalate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "The goal of our project is to implement a reinforcement learning model using Q-learning to optimize navigation and decision-making strategies in a Pac-Man game environment. The agent will be trained to maximize its rewards by collecting pellets like in the game, while avoiding obstacles (ghosts and the walls). The environment is structured as a dynamic Markov Decision Process, where the states represent Pac-Man’s position, pellet locations, and ghost positions and the actions correspond to the movements within the grid.\n",
    "Markov Decision Process provides a mathematical framework for modeling decision-making in stochastic environments, making them well-suited for modeling Pac-Man’s gameplay dynamics. Through Q-learning, the agent will iteratively refine its policy by updating a Q-table based on the rewards and penalties, ultimately optimizing long-term performance. \n",
    "The model will be trained through iterative simulations where it explores different movements, gradually converging to the most optimal policy. The effectiveness of the model will be evaluated based on cumulative reward accumulation, policy convergence, and efficiency in completing different levels. The results of this project aim to demonstrate the feasibility of reinforcement learning in a game-based environment. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Reinforcement learning (RL) allows agents to learn the best techniques through trial and error, it has revolutionized artificial intelligence (AI) in games and decision-making. In contrast to supervised learning, where models are based on labeled datasets, reinforcement learning (RL) enables an agent to engage with its surroundings, get feedback in the form of rewards or penalties, and gradually improve its behavior. From AI used in games to complicated financial and business decision-making, RL's efficacy has been shown in a variety of fields (Sutton & Barto, 2018). The dynamic surroundings, strategic planning requirements, and real-time decision-making limits of games like Pac-Man make them an intriguing RL challenge. Similar to gaming, RL concepts have been used in dynamic pricing models, where businesses adjust product prices in response to shifting market conditions (Keskin & Azevedo, 2017).\n",
    "\n",
    "Pac-Man is a grid-based maze game that introduces multiple competing objectives: maximize score, avoid ghosts, and utilize power pellets strategically. The complexity of the game lies in its non-deterministic ghost movements, limited visibility, and real-time constraints. Classic AI approaches to Pac-Man, such as A pathfinding algorithms and rule-based movement heuristics*, have been widely used but lack adaptability when faced with unexpected situations (Koenig & Likhachev, 2002). RL offers a superior alternative by enabling Pac-Man to learn from experience, adjusting movement strategies dynamically instead of relying on predefined rules. A key factor in RL’s success in game environments is its state-action-reward structure, where the agent processes observations (Pac-Man’s position, pellet locations, ghost behavior) and selects actions (Up, Down, Left, Right) to maximize cumulative rewards. Recent studies have highlighted the potential of deep reinforcement learning (DRL) in mastering game environments with complex decision spaces, further enhancing AI’s ability to generalize across different gameplay scenarios (Mnih et al., 2015).\n",
    "One of the critical challenges in RL for Pac-Man is handling enemy AI behavior, which can range from simple random movements to sophisticated strategies where ghosts actively chase Pac-Man. To make the environment more challenging and realistic, different ghost movement models can be introduced:\n",
    "Random Movement: Ghosts move without a fixed pattern.\n",
    "Rule-Based Chasing: Ghosts follow Pac-Man based on predefined conditions.\n",
    "Probabilistic Mixed Strategies: Ghosts switch between random and targeted movement, making the game environment less predictable.\n",
    "This dynamic opponent behavior requires the RL agent to adapt its decision-making in real-time, enhancing its strategic learning process. Researchers have shown that incorporating multi-agent RL (MARL) in Pac-Man—where both Pac-Man and ghosts operate as independent learning agents—can create emergent behaviors that closely resemble human-like strategies (Hu et al., 1998). This leads to more challenging and engaging gameplay, further pushing the boundaries of AI-driven game intelligence.\n",
    "Beyond video games, RL’s application extends to economic decision-making, particularly in dynamic pricing models. Traditional pricing strategies rely on cost-plus pricing, where businesses apply a markup to production costs. However, this static approach fails to account for demand fluctuations, competitor pricing, and market trends. RL offers an adaptive pricing mechanism where pricing agents continuously adjust product prices based on real-time demand and historical sales data (Keskin & Azevedo, 2017). Similar to how Pac-Man must navigate a maze while responding to evolving threats, pricing agents optimize revenue by reacting to unpredictable market forces. Studies in automated pricing have demonstrated that RL-based models can outperform traditional rule-based methods by identifying optimal pricing policies that maximize long-term profitability while balancing supply-demand dynamics (Misra et al., 2019).\n",
    "\n",
    "By combining RL applications in game AI and economic decision-making, we can draw valuable insights into adaptive learning in competitive environments. The parallel between Pac-Man’s maze navigation and real-world pricing models highlights how RL can optimize decision-making in both virtual and economic landscapes. As AI continues to evolve, its applications in dynamic environments will play an increasingly critical role in gaming, business, and real-time strategic planning.\n",
    "Hu, J., Wellman, M. P., & Kephart, J. O. (1998). Multiagent reinforcement learning: Theoretical framework and an algorithm. Proceedings of the 15th International Conference on Machine Learning (ICML).\n",
    "Keskin, N. B., & Azevedo, E. M. (2017). Dynamic pricing with reinforcement learning: A data-driven approach. Management Science, 63(11), 3760-3777.\n",
    "Koenig, S., & Likhachev, M. (2002). D* Lite. AAAI Conference on Artificial Intelligence.\n",
    "Mnih, V., Kavukcuoglu, K., Silver, D., et al. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533.\n",
    "Misra, K., Schwartz, E. M., & Abernethy, J. (2019). Dynamic online pricing with incomplete information using multi-armed bandits. Marketing Science, 38(2), 226-252.\n",
    "Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT Press.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "We want to build an iteration of the Pacman game that optimizes navigational strategies in a dynamic maze-like environment using reinforcement learning. To make the Pacman game more interesting and interactive, we will be incorporating some special features, specifically power ups that make Pacman immune to the ghosts and able to hunt down the ghosts. Another unique feature that will be added is special characteristics of the ghosts, where one ghost moves more aggressively, following Pacman around, while the second ghost moves randomly.\n",
    "\n",
    "Our goal is to maximize the rewards by collecting pellets while navigating through the maze and learning to avoid obstacles, which are ghosts that penalize the agent. This will be done through the implementation of the Markov Decision Process (MDP) and the Q-learning algorithm, which trains the agent from scratch. MDP is used to define the environment that the agent is trained in, defining the state, action, reward, and transitions for this environment. And the Q-learning algorithm trains the agent to optimize the total rewards as it navigates through the maze using an optimal policy.\n",
    "\n",
    "The problem is: \n",
    "- Quantifiable since the rewards collected through navigating throughout the maze can be tracked.\n",
    "- Measurable since it evaluates the model’s effectiveness throughout the game through the agent’s performance of collecting pellets, avoiding ghosts, and understanding the dynamics of the game.\n",
    "- Replicable, where the model will be trained on various configurations of mazes and pellet placements, where the model has to understand how to navigate through these varying configurations.\n",
    "\n",
    "The challenge of this project comes from the aspect of balancing the exploration and exploitation through an epsilon-greedy strategy of the agent to optimize the rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "The solution to implementing a Pacman agent that optimizes the navigation of the environment, including avoiding ghosts and collecting pellets, involves implementing a Q-learning algorithm with an ε-greedy policy for balancing exploration and exploitation. The agent will update its Q-values using the Bellman equation:\n",
    "\n",
    "State Representation: the environment is like a grid-based maze, which will keep track of the current position of Pacman, the positions of ghosts, the locations of the pellets and power ups, the current state of the game (what condition Pacman is at)\n",
    "\n",
    "Action Space: the available actions for Pacman to take include moving up, down, left, and right, and these actions moves the agent to a new state\n",
    "\n",
    "Reward Function: based on the pellets that are consumed or whether or not a ghost is encountered, there will be rewards that will be given depending on the action taken and the state Pacman is in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "The performance of the model will be evaluated on the following:\n",
    "- Cumulative Reward: Represents the total rewards collected over time\n",
    "- Convergence of Q-values: Represents the reduction in policy over time.\n",
    "- Policy Efficiency: Represents the steps it takes to complete each level.\n",
    "- Comparison to benchmark: Rule-based Pac-Man agent with random/greedy policies. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## Main Point\n",
    "As the Pacman navigated through the maze and learned navigational strategies, the algorithm was able to train Pacman to optimize its navigation throughout the maze, while both collecting pellets and avoiding ghosts.\n",
    "- As the algorithm progresses, it learns to move away from random movements in high epsilon and begin to prioritize pelle collection by finding the nearest pellet to travel to without running into ghosts, where the algorithm is further optimized by becoming more sophisticated in ghost avoidance. The final learning it does is as it is exploring the maze, it finds paths that are the shortest path to the nearest pellet that allows it to maximize the overall reward or score.\n",
    "    - Through the learning process of the algorithm, the algorithm can be seen first starting with a higher epsilon value, meaning a higher exploration value, which is approximately starting at epsilon = 0.606, and gradually decreases to epsilon = 0.01 as it begins to move from greedy, random movements to actions that are based on decisions made from learning the maze and the Q-values.\n",
    " \n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "Using A* heuristic algorithm, we noticed that it was a better performing algorithm for Pacman’s movement. A* algorithm uses Manhattan distance to look for the best paths, which is more optimal than uninformed navigation like BFS/DFS. A* was also more efficient for route navigation as it considers both the distance traveled and the estimated cost to the goal. The algorithm factors in ghost positions, allowing Pacman to avoid dangerous paths. Its adaptability allows for recalculation of the routes when conditions change and there are obstacles blocking paths. \n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Exploration within this algorithm was also guided by heuristics that were based on distance that either gave bonuses for pellets and power-ups and penalties for getting close to ghosts. These additions to the algorithm allowed the Pacman agent to learn how to navigate the maze without running into obstacles and balances random exploration and learned behavior.\n",
    "    - This aspect of using both Q-learning and heuristic functions to optimize Pacman allows\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Implementing different ghosts that challenges Pacman, where one instance, we introduced a ghost that was aggressive and chased Pacman around\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Our baseline model was one that was implemented in the way that once Pacman is caught by a ghost, the game is over and Pacman automatically loses. However, this doesn’t give Pacman a chance to learn the maze at all, therefore, our final model incorporates revival given to Pacman each time it gets caught, where Pacman restarts at its starting position and continues to explore the maze and collect pellets.\n",
    "\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?  Or you compare a completely different approach/alogirhtm to the problem? Whatever, this stuff is just serving suggestions.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Pacman still continues to spazz out\n",
    "    - Can occur due to rapid direction changes - when the agent is constantly recalculating its moves and looking for the most optimal route. This is what causes the oscillation as it's trying to decide between the options.\n",
    "- The algorithm relies on rewards and simple heuristics and lacks a complete understanding of the maze layout, which prevents planning multistep paths.\n",
    "    - The algorithm also only considers relative positions of the ghosts and the pellets within a limited range, and does not consider the full game state.\n",
    "- No transfer of learning\n",
    "    - This means that the agent is trained to navigate a specific maze layout, it learns a policy that is optimized for that exact environment. As a result, this learned knowledge does not transfer when the layout of the maze changes. For each new maze, the agent must go through the entire training process from scratch. \n",
    "\n",
    "\n",
    "\n",
    "### Future work\n",
    "Looking at the limitations and/or the toughest parts of the problem and/or the situations where the algorithm(s) did the worst... is there something you'd like to try to make these better.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination.\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n",
    "\n",
    "\n",
    "## OURS\n",
    "<a name=\"shillernote\"></a>[<sup>[1]</sup>](#shiller) Shiller, Robert J. *Narrative Economics: How Stories Go Viral and Drive Economic Events.* Princeton University Press, 2019. [https://press.princeton.edu/books/hardcover/9780691182292/narrative-economics](https://press.princeton.edu/books/hardcover/9780691182292/narrative-economics).\n",
    "\n",
    "<a name=\"martineznote\"></a>[<sup>[2]</sup>](#martinez) Martinez, Blake. *\"How I Made Millions Selling Pokémon Cards After Leaving the NFL.\"* CNBC, 26 Apr. 2023. [https://www.cnbc.com/2023/04/26/blake-martinez-pokemon-card-side-hustle-company-brings-in-millions.html](https://www.cnbc.com/2023/04/26/blake-martinez-pokemon-card-side-hustle-company-brings-in-millions.html). Accessed 14 Feb. 2025.\n",
    "\n",
    "<a name=\"platformsnote\"></a>[<sup>[3]</sup>](#platforms) Rochet, Jean-Charles, and Jean Tirole. *\"Platform Competition in Two-Sided Markets.\"* *Journal of the European Economic Association*, vol. 1, no. 4, 2003, pp. 990-1029. [https://academic.oup.com/jeea/article/1/4/990/2280902](https://academic.oup.com/jeea/article/1/4/990/2280902).\n",
    "\n",
    "<a name=\"networknote\"></a>[<sup>[4]</sup>](#network) Evans, David S., and Richard Schmalensee. *\"The Economics of Two-Sided Markets.\"* *Review of Network Economics*, vol. 6, no. 2, 2007, pp. 1-26.\n",
    "\n",
    "<a name=\"ai-pricingnote\"></a>[<sup>[5]</sup>](#ai-pricing) Bertsimas, Dimitris, and Nathan Kallus. *\"From Predictive to Prescriptive Analytics.\"* *Management Science*, vol. 65, no. 3, 2019, pp. 1027-1049. [https://pubsonline.informs.org/doi/10.1287/mnsc.2018.3253](https://pubsonline.informs.org/doi/10.1287/mnsc.2018.3253).\n",
    "\n",
    "<a name=\"markovnote\"></a>[<sup>[6]</sup>](#markov) Puterman, Martin L. *Markov Decision Processes: Discrete Stochastic Dynamic Programming.* John Wiley & Sons, 1994. [https://onlinelibrary.wiley.com/doi/chapter-epub/10.1002/9780470316887.fmatter](https://onlinelibrary.wiley.com/doi/chapter-epub/10.1002/9780470316887.fmatter).\n",
    "\n",
    "<a name=\"qlearningnote\"></a>[<sup>[7]</sup>](#qlearning) Watkins, Christopher J. C. H., and Peter Dayan. *\"Q-learning: Model-Free Reinforcement Learning.\"* *Machine Learning Journal*, vol. 8, no. 3-4, 1992, pp. 279-292. [https://link.springer.com/article/10.1007/BF00992698](https://link.springer.com/article/10.1007/BF00992698).\n",
    "\n",
    "<a name=\"ai-commercenote\"></a>[<sup>[8]</sup>](#ai-commerce) Tesauro, Gerald, and Jeffrey O. Kephart. *\"Pricing Strategies Using Q-Learning in E-Commerce.\"* *AAAI Conference on Artificial Intelligence*, 2002. [https://www.researchgate.net/publication/2820310_Pricing_in_Agent_Economies_Using_Multi-Agent_Q-Learning](https://www.researchgate.net/publication/2820310_Pricing_in_Agent_Economies_Using_Multi-Agent_Q-Learning).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
